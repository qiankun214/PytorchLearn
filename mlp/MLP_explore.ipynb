{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP探索\n",
    "- 探索削减精度对预测的影响\n",
    "- 探索参数成倍变化对预测的影响\n",
    "- 探索训练时使用批标准化，预测时不使用批标准化对预测的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0 : 0.11\n",
      "200 : 0.9\n",
      "400 : 0.96\n",
      "0 : 0.96\n",
      "200 : 0.96\n",
      "400 : 0.97\n"
     ]
    }
   ],
   "source": [
    "import torch as pt\n",
    "import torchvision as ptv\n",
    "import numpy as np\n",
    "train_set = ptv.datasets.MNIST(\"../../pytorch_database/mnist/train\",train=True,transform=ptv.transforms.ToTensor(),download=True)\n",
    "test_set = ptv.datasets.MNIST(\"../../pytorch_database/mnist/test\",train=False,transform=ptv.transforms.ToTensor(),download=True)\n",
    "train_dataset = pt.utils.data.DataLoader(train_set,batch_size=100)\n",
    "test_dataset = pt.utils.data.DataLoader(test_set,batch_size=100)\n",
    "\n",
    "def AccuarcyCompute(pred,label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "class MLP(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.norm1 = pt.nn.BatchNorm1d(512,momentum=0.1,affine=False)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.norm2 = pt.nn.BatchNorm2d(128,momentum=0.1,affine=False)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(self.norm1(self.fc1(din)))\n",
    "        dout = pt.nn.functional.relu(self.norm2(self.fc2(dout)))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model_norm = MLP().cuda()\n",
    "print(model_norm)\n",
    "\n",
    "optimizer = pt.optim.SGD(model_norm.parameters(),lr=0.01,momentum=0.9)\n",
    "lossfunc = pt.nn.CrossEntropyLoss().cuda()\n",
    "for x in range(2):\n",
    "    for i,data in enumerate(train_dataset):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        (inputs,labels) = data\n",
    "        inputs = pt.autograd.Variable(inputs).cuda()\n",
    "        labels = pt.autograd.Variable(labels).cuda()\n",
    "    \n",
    "        outputs = model_norm(inputs)\n",
    "    \n",
    "        loss = lossfunc(outputs,labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 200 == 0:\n",
    "            print(i,\":\",AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965700003505\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_norm(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))\n",
    "pt.save(model_norm.state_dict(),\"../../pytorch_model/mlp/explore_params/mlp_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 削减精度对网络的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0.962100004554\n"
     ]
    }
   ],
   "source": [
    "mlp_low = MLP().cuda()\n",
    "mlp_low.load_state_dict(pt.load(\"../../pytorch_model/mlp/explore_params/mlp_params.pt\"))\n",
    "print(mlp_low)\n",
    "for name,f in mlp_low.named_parameters():\n",
    "    f.data = f.data * 100\n",
    "    f.data = f.data.int().float()\n",
    "    f.data = f.data / 100\n",
    "#     print(name,f)\n",
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = mlp_low(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0.0974000002816\n"
     ]
    }
   ],
   "source": [
    "mlp_low = MLP().cuda()\n",
    "mlp_low.load_state_dict(pt.load(\"../../pytorch_model/mlp/explore_params/mlp_params.pt\"))\n",
    "print(mlp_low)\n",
    "for name,f in mlp_low.named_parameters():\n",
    "    f.data = f.data * 10\n",
    "    f.data = f.data.int().float()\n",
    "    f.data = f.data / 10\n",
    "#     print(name,f)\n",
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = mlp_low(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的比较发现，mlp中的精度削减可能存在一个阈值。以上面为例，削减参数精度为小数点后2位时对结果几乎没有影响；而削减为小数点后1位时，结果已无法接受"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数成倍变化对网络是否有影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0.0979999999329\n"
     ]
    }
   ],
   "source": [
    "mlp_double = MLP().cuda()\n",
    "mlp_double.load_state_dict(pt.load(\"../../pytorch_model/mlp/explore_params/mlp_params.pt\"))\n",
    "print(mlp_double)\n",
    "for name,f in mlp_double.named_parameters():\n",
    "    f.data = f.data * 2\n",
    "    f.data = f.data.int().float()\n",
    "#     print(name,f)\n",
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = mlp_double(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上看出参数成倍变化可能对结果造成灾难性影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测时去除批标准化层对结果的影响\n",
    "### 直接去除批标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_no (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_no(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_no,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(self.fc1(din))\n",
    "        dout = pt.nn.functional.relu(self.fc2(dout))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model_no = MLP_no().cuda()\n",
    "print(model_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_double = MLP().cuda()\n",
    "mlp_double.load_state_dict(pt.load(\"../../pytorch_model/mlp/explore_params/mlp_params.pt\"))\n",
    "param_dict = {}\n",
    "for name,f in mlp_double.named_parameters():\n",
    "    param_dict[name] = f\n",
    "# print(param_dict)\n",
    "for name,f in model_no.named_parameters():\n",
    "    if name in param_dict:\n",
    "        f.data = param_dict[name].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809399998188\n"
     ]
    }
   ],
   "source": [
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_no(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，在预测过程中将批标准化移除后，会有一部分性能损失，但是还没到灾难的地步\n",
    "### 使用减去平均值的方法代替批标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_avg (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0.809099998474\n"
     ]
    }
   ],
   "source": [
    "class MLP_avg(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_avg,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = self.fc1(din)\n",
    "        dout.data = self.sub_average(dout)\n",
    "        dout = pt.nn.functional.relu(dout)\n",
    "        dout = self.fc2(dout)\n",
    "        dout.data = self.sub_average(dout)\n",
    "        dout = pt.nn.functional.relu(dout)\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "    \n",
    "    def sub_average(self,din):\n",
    "        average = pt.sum(din)\n",
    "        num = 0\n",
    "        for i in din.size():\n",
    "            num += i\n",
    "        return din.data.sub_(din.data / num)\n",
    "\n",
    "model_avg = MLP_avg().cuda()\n",
    "print(model_avg)\n",
    "\n",
    "mlp_double = MLP().cuda()\n",
    "mlp_double.load_state_dict(pt.load(\"../../pytorch_model/mlp/explore_params/mlp_params.pt\"))\n",
    "param_dict = {}\n",
    "for name,f in mlp_double.named_parameters():\n",
    "    param_dict[name] = f\n",
    "# print(param_dict)\n",
    "for name,f in model_avg.named_parameters():\n",
    "    if name in param_dict:\n",
    "        f.data = param_dict[name].data\n",
    "\n",
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_avg(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用减去平均值的方法代替批标准化对结果没有提升\n",
    "### 使用normalize函数代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_normalize (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_normalize(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_normalize,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(pt.nn.functional.normalize(self.fc1(din),p=2))\n",
    "        dout = pt.nn.functional.relu(pt.nn.functional.normalize(self.fc2(dout),p=2))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model_normalize = MLP_normalize().cuda()\n",
    "print(model_normalize)\n",
    "\n",
    "optimizer = pt.optim.SGD(model_normalize.parameters(),lr=0.01,momentum=0.9)\n",
    "lossfunc = pt.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.92\n",
      "200 : 0.86\n",
      "400 : 0.91\n",
      "0 : 0.92\n",
      "200 : 0.86\n",
      "400 : 0.91\n",
      "0 : 0.92\n",
      "200 : 0.86\n",
      "400 : 0.91\n",
      "0 : 0.92\n",
      "200 : 0.86\n",
      "400 : 0.91\n",
      "0 : 0.93\n",
      "200 : 0.86\n",
      "400 : 0.92\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    for i,data in enumerate(train_dataset):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        (inputs,labels) = data\n",
    "        inputs = pt.autograd.Variable(inputs).cuda()\n",
    "        labels = pt.autograd.Variable(labels).cuda()\n",
    "    \n",
    "        outputs = model_normalize(inputs)\n",
    "    \n",
    "        loss = lossfunc(outputs,labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 200 == 0:\n",
    "            print(i,\":\",AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886699998379\n"
     ]
    }
   ],
   "source": [
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_normalize(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，使用`torch.nn.functional.normalize()`函数代替批标准化层后，精度有一定下降，性能较直接移除而言稍高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "- 移除一定的精度MLP网络的性能影响并不大\n",
    "- 参数成倍上升或下降对性能造成灾难性影响\n",
    "- 直接或使用易于计算的函数代替批标准化对模型性能造成损失"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
