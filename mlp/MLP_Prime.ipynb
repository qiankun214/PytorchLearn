{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP中实现dropout，批标准化\n",
    "## 基本网络代码\n",
    "- 三层MLP\n",
    "- 使用MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n",
      "0 : 0.1\n",
      "100 : 0.2\n",
      "200 : 0.34\n",
      "300 : 0.17\n",
      "400 : 0.51\n",
      "500 : 0.52\n",
      "0 : 0.79\n",
      "100 : 0.77\n",
      "200 : 0.69\n",
      "300 : 0.75\n",
      "400 : 0.85\n",
      "500 : 0.85\n",
      "0 : 0.88\n",
      "100 : 0.8\n",
      "200 : 0.76\n",
      "300 : 0.79\n",
      "400 : 0.85\n",
      "500 : 0.85\n",
      "0 : 0.89\n",
      "100 : 0.81\n",
      "200 : 0.77\n",
      "300 : 0.82\n",
      "400 : 0.85\n",
      "500 : 0.86\n"
     ]
    }
   ],
   "source": [
    "import torch as pt\n",
    "import torchvision as ptv\n",
    "import numpy as np\n",
    "\n",
    "train_set = ptv.datasets.MNIST(\"../../pytorch_database/mnist/train\",train=True,transform=ptv.transforms.ToTensor(),download=True)\n",
    "test_set = ptv.datasets.MNIST(\"../../pytorch_database/mnist/test\",train=False,transform=ptv.transforms.ToTensor(),download=True)\n",
    "train_dataset = pt.utils.data.DataLoader(train_set,batch_size=100)\n",
    "test_dataset = pt.utils.data.DataLoader(test_set,batch_size=100)\n",
    "\n",
    "class MLP(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(self.fc1(din))\n",
    "        dout = pt.nn.functional.relu(self.fc2(dout))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model = MLP().cuda()\n",
    "print(model)\n",
    "\n",
    "# loss func and optim\n",
    "optimizer = pt.optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "lossfunc = pt.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# accuarcy\n",
    "def AccuarcyCompute(pred,label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "for x in range(4):\n",
    "    for i,data in enumerate(train_dataset):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        (inputs,labels) = data\n",
    "        inputs = pt.autograd.Variable(inputs).cuda()\n",
    "        labels = pt.autograd.Variable(labels).cuda()\n",
    "    \n",
    "        outputs = model(inputs)\n",
    "    \n",
    "        loss = lossfunc(outputs,labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(i,\":\",AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增加批标准化\n",
    "批标准化是添加在激活函数之前，使用标准化的方式将输入处理到一个区域内或者近似平均的分布在一个区域内\n",
    "在pytorch中，使用`torch.nn.BatchNorm1/2/3d（）`函数表示一个批标准化层，使用方法与其它层类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.norm1 = pt.nn.BatchNorm1d(512,momentum=0.5)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.norm2 = pt.nn.BatchNorm2d(128,momentum=0.5)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(self.norm1(self.fc1(din)))\n",
    "        dout = pt.nn.functional.relu(self.norm2(self.fc2(dout)))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model_norm = MLP().cuda()\n",
    "print(model_norm)\n",
    "\n",
    "optimizer = pt.optim.SGD(model_norm.parameters(),lr=0.01,momentum=0.9)\n",
    "lossfunc = pt.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.2\n",
      "200 : 0.69\n",
      "400 : 0.89\n",
      "0 : 0.96\n",
      "200 : 0.95\n",
      "400 : 0.97\n",
      "0 : 0.97\n",
      "200 : 0.96\n",
      "400 : 0.99\n",
      "0 : 0.97\n",
      "200 : 0.97\n",
      "400 : 0.99\n",
      "0 : 0.97\n",
      "200 : 0.97\n",
      "400 : 0.99\n",
      "0 : 0.97\n",
      "200 : 0.98\n",
      "400 : 0.99\n"
     ]
    }
   ],
   "source": [
    "for x in range(6):\n",
    "    for i,data in enumerate(train_dataset):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        (inputs,labels) = data\n",
    "        inputs = pt.autograd.Variable(inputs).cuda()\n",
    "        labels = pt.autograd.Variable(labels).cuda()\n",
    "    \n",
    "        outputs = model_norm(inputs)\n",
    "    \n",
    "        loss = lossfunc(outputs,labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 200 == 0:\n",
    "            print(i,\":\",AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976300007105\n"
     ]
    }
   ],
   "source": [
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_norm(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与不使用批标准化的网络（准确率93%左右）相比，使用批标准化的网络准确率由明显的提高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout\n",
    "dropout是一种常见的防止过拟合的方法，通过将网络中的神经元随机的置0来达到防止过拟合的目的\n",
    "pytorch中使用`torch.nn.Dropout()`和`torch.nn.Dropout2/3d()`函数构造，且该层只在训练中起作用，在预测时dropout将不会工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (fc1): Linear (784 -> 512)\n",
      "  (drop1): Dropout (p = 0.6)\n",
      "  (fc2): Linear (512 -> 128)\n",
      "  (drop2): Dropout (p = 0.6)\n",
      "  (fc3): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = pt.nn.Linear(784,512)\n",
    "        self.drop1 = pt.nn.Dropout(0.6)\n",
    "        self.fc2 = pt.nn.Linear(512,128)\n",
    "        self.drop2 = pt.nn.Dropout(0.6)\n",
    "        self.fc3 = pt.nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,din):\n",
    "        din = din.view(-1,28*28)\n",
    "        dout = pt.nn.functional.relu(self.drop1(self.fc1(din)))\n",
    "        dout = pt.nn.functional.relu(self.drop2(self.fc2(dout)))\n",
    "        return pt.nn.functional.softmax(self.fc3(dout))\n",
    "model_drop = MLP().cuda()\n",
    "print(model_drop)\n",
    "optimizer = pt.optim.SGD(model_drop.parameters(),lr=0.01,momentum=0.9)\n",
    "lossfunc = pt.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.11\n",
      "200 : 0.25\n",
      "400 : 0.32\n",
      "0 : 0.5\n",
      "200 : 0.51\n",
      "400 : 0.74\n",
      "0 : 0.8\n",
      "200 : 0.68\n",
      "400 : 0.8\n",
      "0 : 0.86\n",
      "200 : 0.74\n",
      "400 : 0.85\n",
      "0 : 0.88\n",
      "200 : 0.78\n",
      "400 : 0.8\n",
      "0 : 0.9\n",
      "200 : 0.75\n",
      "400 : 0.83\n"
     ]
    }
   ],
   "source": [
    "for x in range(6):\n",
    "    for i,data in enumerate(train_dataset):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        (inputs,labels) = data\n",
    "        inputs = pt.autograd.Variable(inputs).cuda()\n",
    "        labels = pt.autograd.Variable(labels).cuda()\n",
    "    \n",
    "        outputs = model_drop(inputs)\n",
    "    \n",
    "        loss = lossfunc(outputs,labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % 200 == 0:\n",
    "            print(i,\":\",AccuarcyCompute(outputs,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840299996734\n"
     ]
    }
   ],
   "source": [
    "accuarcy_list = []\n",
    "for i,(inputs,labels) in enumerate(test_dataset):\n",
    "    inputs = pt.autograd.Variable(inputs).cuda()\n",
    "    labels = pt.autograd.Variable(labels).cuda()\n",
    "    outputs = model_drop(inputs)\n",
    "    accuarcy_list.append(AccuarcyCompute(outputs,labels))\n",
    "print(sum(accuarcy_list) / len(accuarcy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，dropout对于系统性能的还是有比较大的影响的，对于这种微型网络来说，泛化能力的提升并不明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 疑问\n",
    "当批标准化和dropout同时存在时，这两个层次的相互位置该如何考虑\n",
    "- -> dropout->norm->function?\n",
    "- -> norm->dropout->function?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
